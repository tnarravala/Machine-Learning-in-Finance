{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aNI_8snBObvX"
   },
   "outputs": [],
   "source": [
    "# Interpretability_5_3-5(Solutions to Exercises 5.3, 5.4 and 5.5)\n",
    "# Author: Matthew Dixon\n",
    "# Version: 1.0 (08.09.2019)\n",
    "# License: MIT\n",
    "# Email: matthew.dixon@iit.edu\n",
    "# Notes: tested on Mac OS X with Python 3.6\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# Dixon M.F. I. Halperin and P. Bilokon, Machine Learning in Finance: From Theory to Practice, Springer Graduate textbook Series, 2020. \n",
    "# Solution courtesy of Shengnan Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "ilBxVge5Obvg",
    "outputId": "d78b90ab-4625-4f76-8f42-e23b76f43003"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.diagnostic as tds\n",
    "from statsmodels.api import add_constant\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYgbL8x7Obvm"
   },
   "source": [
    "# Simple Data Generation Process (DGP)\n",
    "\n",
    "$Y=X_1+X_2 + \\epsilon, ~X_1, X_2, \\epsilon \\sim N(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVrf3OuiObvp"
   },
   "outputs": [],
   "source": [
    "M = 5000 # Number of sample \n",
    "np.random.seed(7) # set the seed for reproducebility of results\n",
    "X = np.zeros(shape=(M,2))\n",
    "X[:int(M/2),0]= np.random.randn(int(M/2))\n",
    "X[:int(M/2),1]= np.random.randn(int(M/2))\n",
    "\n",
    "X[int(M/2):,0]= -X[:int(M/2),0] # zero bias of mean with antithetic sampling\n",
    "X[int(M/2):,1]= -X[:int(M/2),1]\n",
    "\n",
    "\n",
    "eps= np.random.randn(M)\n",
    "Y= 1.0*X[:,0] + 1.0*X[:,1] + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yie3-2_5Obv1"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-HKYZnwObwC"
   },
   "source": [
    "# Exercise 5.3. Compare with a FFW Neural Network with one hidden layer (tanh activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From running the code, it is evident that the activation function that should be used for interpretability is tanh.  When the number of hidden units is low, all three cases (no activation, tanh activation, and ReLU activation) provide fairly reasonable results, with means somewhat close to the true sensitivity values and small standard deviations.  \n",
    "However, as the number of hidden units increases, tanh is the activation function that remains the best at interpreting the sensitivities.  Using tanh activation, the means get closer to the true values of the sensitivities, and the standard deviations remain small.  On the contrary, when ReLU activation is used, it can be seen that the sensitivity approximations are closer to the true values when the number of hidden units are small.  As the number of hidden units increases (above 100), the mean approximations do not capture the true sensitivities as well as tanh activation does.  As a result, tanh activation should be used over ReLU and no activation when it comes to capturing the sensitivities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_BKDs-aObwD"
   },
   "outputs": [],
   "source": [
    "# number of hidden neurons\n",
    "n=[10,50,100,200,500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAcGW4Z9uDHu"
   },
   "outputs": [],
   "source": [
    "# Compute the sensitivities\n",
    "# Assume that the activation function is tanh\n",
    "def sensitivities_t(lm, X):\n",
    "\n",
    "    W1=lm.model.get_weights()[0]\n",
    "    b1=lm.model.get_weights()[1]\n",
    "    W2=lm.model.get_weights()[2]\n",
    "    b2=lm.model.get_weights()[3]\n",
    "\n",
    "\n",
    "    M = np.shape(X)[0]\n",
    "    p = np.shape(X)[1]\n",
    "\n",
    "    beta=np.array([0]*M*(p+1), dtype='float32').reshape(M,p+1)\n",
    "\n",
    "    beta[:,0]= (np.dot(np.transpose(W2),np.tanh(b1)) + b2)[0] # intercept \\beta_0= F_{W,b}(0)\n",
    "    for i in range(M):\n",
    "\n",
    "      Z1 = np.tanh(np.dot(np.transpose(W1),np.transpose(X[i,])) + b1) \n",
    "\n",
    "      D = np.diag(1-Z1**2)\n",
    "\n",
    "      for j in range(p):  \n",
    "          beta[i,j+1]=np.dot(np.transpose(W2),np.dot(D,W1[j]))\n",
    "\n",
    "    return(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, below is a table that shows the means and standard deviations of the sensitivities as the number of hidden units is increased.  This is shown for one hidden layer with tanh activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "cpubFhDsObwE",
    "outputId": "ca639f35-039c-421f-f3af-9871367a31f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00057: early stopping\n",
      "Epoch 00022: early stopping\n",
      "Epoch 00015: early stopping\n",
      "Epoch 00021: early stopping\n",
      "Epoch 00051: early stopping\n",
      "   number_of_neurons     beta0     beta1     beta2\n",
      "0               10.0 -0.002274  0.994728  0.967177\n",
      "1               50.0  0.017609  0.990028  0.987109\n",
      "2              100.0  0.074269  0.952894  0.963494\n",
      "3              200.0  0.007517  0.994349  0.969850\n",
      "4              500.0  0.000408  1.010127  1.002024\n",
      "   number_of_neurons         beta0     beta1     beta2\n",
      "0               10.0  1.070991e-07  0.058954  0.057510\n",
      "1               50.0  2.831221e-07  0.046515  0.045881\n",
      "2              100.0  3.844407e-06  0.032985  0.033852\n",
      "3              200.0  7.497021e-08  0.031956  0.031605\n",
      "4              500.0  9.458969e-09  0.038202  0.038744\n"
     ]
    }
   ],
   "source": [
    "array_mean = []\n",
    "array_std = []\n",
    "for k in range(len(n)):\n",
    "  \n",
    "  # with non-linear activation\n",
    "  def linear_NN1_model_act(l1_reg=0.0):    \n",
    "      model = Sequential()\n",
    "      model.add(Dense(n[k], input_dim=2, kernel_initializer='normal', activation='tanh'))\n",
    "      model.add(Dense(1, kernel_initializer='normal')) \n",
    "      model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "      return model\n",
    "   \n",
    "  lm = KerasRegressor(build_fn=linear_NN1_model_act, epochs=100, batch_size=10, verbose=0, callbacks=[es])\n",
    "  lm.fit(X,Y)\n",
    "  \n",
    "  beta=sensitivities_t(lm, X)\n",
    "  # Check that the intercept is close to one and the coefficients are close to one\n",
    "  array_mean.append(int(n[k]))\n",
    "  array_mean.extend(np.mean(beta, axis=0).tolist())\n",
    "  array_std.append(int(n[k]))\n",
    "  array_std.extend(np.std(beta, axis=0).tolist())\n",
    "  \n",
    "pd_mean = pd.DataFrame(np.reshape(array_mean,(len(n),4)),columns=['number_of_neurons','beta0','beta1','beta2'])\n",
    "pd_std = pd.DataFrame(np.reshape(array_std,(len(n),4)),columns=['number_of_neurons','beta0','beta1','beta2'])\n",
    "print(pd_mean)\n",
    "print(pd_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCLXTes4zrBu"
   },
   "source": [
    "# Compare with a FFW Neural Network with one hidden layer (ReLU activated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, below is a table that shows the means and standard deviations of the sensitivities as the number of hidden units is increased.  This is shown for one hidden layer with ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2NriatFzqYV"
   },
   "outputs": [],
   "source": [
    "# Compute the sensitivities\n",
    "# Assume that the activation function is tanh\n",
    "def sensitivities_r(lm, X):\n",
    "\n",
    "    W1=lm.model.get_weights()[0]\n",
    "    b1=lm.model.get_weights()[1]\n",
    "    W2=lm.model.get_weights()[2]\n",
    "    b2=lm.model.get_weights()[3]\n",
    "\n",
    "\n",
    "    M = np.shape(X)[0]\n",
    "    p = np.shape(X)[1]\n",
    "\n",
    "    beta=np.array([0]*M*(p+1), dtype='float32').reshape(M,p+1)\n",
    "\n",
    "    beta[:,0]= (np.dot(np.transpose(W2),np.maximum(b1,0)) + b2)[0] # intercept \\beta_0= F_{W,b}(0)\n",
    "    for i in range(M):\n",
    "\n",
    "      Z1 = np.maximum(np.dot(np.transpose(W1),np.transpose(X[i,])) + b1,0) \n",
    "\n",
    "      D = np.diag(np.sign(Z1))  \n",
    "\n",
    "      for j in range(p):  \n",
    "          beta[i,j+1]=np.dot(np.transpose(W2),np.dot(D,W1[j]))\n",
    "\n",
    "    return(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "g8_hm-082Tup",
    "outputId": "35a17d95-2cba-442c-8b7d-146ddca28ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00050: early stopping\n",
      "Epoch 00042: early stopping\n",
      "Epoch 00036: early stopping\n",
      "Epoch 00023: early stopping\n",
      "Epoch 00033: early stopping\n",
      "   number_of_neurons     beta0     beta1     beta2\n",
      "0               10.0  0.027086  1.001396  1.012632\n",
      "1               50.0  0.018870  0.945165  0.966890\n",
      "2              100.0 -0.001559  1.003370  1.010414\n",
      "3              200.0  0.013321  1.027624  0.989039\n",
      "4              500.0  0.016759  0.999056  1.116667\n",
      "   number_of_neurons         beta0     beta1     beta2\n",
      "0               10.0  6.705523e-07  0.087771  0.099541\n",
      "1               50.0  6.109476e-07  0.109643  0.132735\n",
      "2              100.0  7.392217e-08  0.125465  0.133331\n",
      "3              200.0  3.985938e-07  0.126030  0.139356\n",
      "4              500.0  5.830266e-07  0.118538  0.159243\n"
     ]
    }
   ],
   "source": [
    "array_mean = []\n",
    "array_std = []\n",
    "for k in range(len(n)):\n",
    "  \n",
    "  # with non-linear activation\n",
    "  def linear_NN1_model_act(l1_reg=0.0):    \n",
    "      model = Sequential()\n",
    "      model.add(Dense(n[k], input_dim=2, kernel_initializer='normal', activation='relu'))\n",
    "      model.add(Dense(1, kernel_initializer='normal')) \n",
    "      model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "      return model\n",
    "   \n",
    "  lm = KerasRegressor(build_fn=linear_NN1_model_act, epochs=100, batch_size=10, verbose=0, callbacks=[es])\n",
    "  lm.fit(X,Y)\n",
    "  \n",
    "  beta=sensitivities_r(lm, X)\n",
    "  # Check that the intercept is close to one and the coefficients are close to one\n",
    "  array_mean.append(int(n[k]))\n",
    "  array_mean.extend(np.mean(beta, axis=0).tolist())\n",
    "  array_std.append(int(n[k]))\n",
    "  array_std.extend(np.std(beta, axis=0).tolist())\n",
    "  \n",
    "pd_mean = pd.DataFrame(np.reshape(array_mean,(len(n),4)),columns=['number_of_neurons','beta0','beta1','beta2'])\n",
    "pd_std = pd.DataFrame(np.reshape(array_std,(len(n),4)),columns=['number_of_neurons','beta0','beta1','beta2'])\n",
    "print(pd_mean)\n",
    "print(pd_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0C8PthM4PqU"
   },
   "source": [
    "# Exercise 5.4: Test the sensitivities function with L hidden layers (tanh activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Gq3JR1x4x-z"
   },
   "outputs": [],
   "source": [
    "# Compute the sensitivities for a feedforward architecture with L layers\n",
    "# Assume that the activation function is tanh\n",
    "def deep_sensitivities(lm, X):\n",
    "    \n",
    "    para = lm.model.get_weights()\n",
    "    W = [] # store weights\n",
    "    b = [] # store biases\n",
    "    for index in range(len(para)):\n",
    "      if index%2 == 0:\n",
    "        W.append(para[index])\n",
    "      else: \n",
    "        b.append(para[index])\n",
    "    M = np.shape(X)[0]\n",
    "    p = np.shape(X)[1]\n",
    "\n",
    "    beta=np.array([0]*M*(p+1), dtype='float32').reshape(M,p+1)\n",
    "    \n",
    "    beta0 = np.dot(np.transpose(W[1]),np.tanh(b[0])) + b[1]\n",
    "    for i in range(2,L):\n",
    "      beta0 = np.dot(np.transpose(W[i]),np.tanh(beta0)) + b[i]\n",
    "    beta[:,0] = beta0[0]\n",
    "    \n",
    "    for i in range(M):\n",
    "\n",
    "        Z1 = np.tanh(np.dot(np.transpose(W[0]),np.transpose(X[i,])) + b[0]) \n",
    "\n",
    "        D1 = np.diag(1-Z1**2)\n",
    "\n",
    "        for j in range(p): \n",
    "        \n",
    "            Z = Z1\n",
    "\n",
    "            temp = np.dot(np.transpose(W[1]),np.dot(D1,W[0][j]))\n",
    "\n",
    "            for l in range(1,L-1):\n",
    "\n",
    "                Z = np.tanh(np.dot(np.transpose(W[l]),np.transpose(Z)) + b[l]) \n",
    "                D = np.diag(1-Z**2)\n",
    "                temp = np.dot(np.transpose(W[l+1]),np.dot(D,temp))\n",
    "            beta[i,j+1] = temp\n",
    "                                                                 \n",
    "    return (beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "8lkV7Ydx9Lf3",
    "outputId": "18e56241-fa69-4bac-e2dc-436bec1e5b39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00034: early stopping\n",
      "[[0.07728125 1.0203655  1.0580494 ]\n",
      " [0.07728125 1.0396371  1.0720679 ]\n",
      " [0.07728125 1.0998473  1.1350926 ]\n",
      " ...\n",
      " [0.07728125 0.9173199  0.95067984]\n",
      " [0.07728125 0.9627277  0.9931845 ]\n",
      " [0.07728125 1.109096   1.1446314 ]]\n"
     ]
    }
   ],
   "source": [
    "n = 10 # number of units in each hidden layer\n",
    "L = 4 # number of layers, including one output layer\n",
    "# with non-linear activation\n",
    "def linear_NNL_model_act(l1_reg=0.0):    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n, input_dim=2, kernel_initializer='normal', activation='tanh'))\n",
    "    for l in range(2,L):\n",
    "      model.add(Dense(n, kernel_initializer='normal', activation='tanh'))\n",
    "    model.add(Dense(1, kernel_initializer='normal')) \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "   \n",
    "lm = KerasRegressor(build_fn=linear_NNL_model_act, epochs=100, batch_size=10, verbose=0, callbacks=[es])\n",
    "lm.fit(X,Y)\n",
    "\n",
    "beta = deep_sensitivities(lm, X)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "bVpZ24LQkqDb",
    "outputId": "f2f17fe1-267a-4398-fc44-fc3b7d90b6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07727869 1.0217295  1.0542846 ]\n",
      "[2.5629997e-06 1.0636521e-01 1.1001225e-01]\n"
     ]
    }
   ],
   "source": [
    "# Check that the intercept is close to one and the coefficients are close to one\n",
    "print(np.mean(beta, axis=0))\n",
    "print(np.std(beta, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YNOMW_BLf7Ps"
   },
   "source": [
    "# Exercise 5.5: Compare the sensitivities with fixed hidden units (tanh activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we fix the total number of hidden units but change the number of hidden layers.  This allows us to make a conclusion about how the mean and standard deviations of the sensitivities behave as the number of hidden layers is increased.  \n",
    "Through running the code, it is evident that as the number of hidden layers is increased, the approximations of the sensitivities do not improve.  The means do not converge to the true value of the sensitivities, and the standard deviations appear to grow.  Thus, one might tentatively conclude that keeping the total number of hidden units fixed, the higher the number of hidden layers, the least certain the approximation of the sensitivities will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "8aU4oQJ4gd-V",
    "outputId": "92bf216d-f983-4852-f694-f43a606a287b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: early stopping\n",
      "Epoch 00022: early stopping\n",
      "Epoch 00026: early stopping\n",
      "Epoch 00029: early stopping\n",
      "   number_of_hidden_layers     beta0     beta1     beta2\n",
      "0                      1.0  0.026048  0.986498  0.977701\n",
      "1                      2.0  0.008938  1.067570  1.031970\n",
      "2                      5.0 -0.052291  1.039532  0.988219\n",
      "3                     10.0 -0.164735  1.008504  0.989027\n",
      "   number_of_hidden_layers         beta0     beta1     beta2\n",
      "0                      1.0  8.233117e-07  0.028760  0.029629\n",
      "1                      2.0  4.926731e-07  0.081046  0.078865\n",
      "2                      5.0  5.774097e-07  0.119299  0.113469\n",
      "3                     10.0  3.844407e-06  0.186785  0.183237\n"
     ]
    }
   ],
   "source": [
    "total = 200 # fixed total number of hidden units \n",
    "layers = [1, 2, 5, 10] # number of hidden layers\n",
    "array_mean = []\n",
    "array_std = []\n",
    "for k in range(len(layers)):\n",
    "    n = int(total/layers[k]) # number of units in each hidder layer\n",
    "    L = layers[k]+1 # number of layers, including one output layer\n",
    "    # with non-linear activation\n",
    "    def linear_NNL_model_act(l1_reg=0.0):    \n",
    "        model = Sequential()\n",
    "        model.add(Dense(n, input_dim=2, kernel_initializer='normal', activation='tanh'))\n",
    "        for l in range(2,L):\n",
    "          model.add(Dense(n, kernel_initializer='normal', activation='tanh'))\n",
    "        model.add(Dense(1, kernel_initializer='normal')) \n",
    "        model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "        return model\n",
    "\n",
    "    lm = KerasRegressor(build_fn=linear_NNL_model_act, epochs=100, batch_size=10, verbose=0, callbacks=[es])\n",
    "    lm.fit(X,Y)\n",
    "\n",
    "    beta = deep_sensitivities(lm, X)\n",
    "    # Check that the intercept is close to one and the coefficients are close to one\n",
    "    array_mean.append(layers[k])\n",
    "    array_mean.extend(np.mean(beta, axis=0).tolist())\n",
    "    array_std.append(layers[k])\n",
    "    array_std.extend(np.std(beta, axis=0).tolist())\n",
    "  \n",
    "pd_mean = pd.DataFrame(np.reshape(array_mean,(len(layers),4)),columns=['number_of_hidden_layers','beta0','beta1','beta2'])\n",
    "pd_std = pd.DataFrame(np.reshape(array_std,(len(layers),4)),columns=['number_of_hidden_layers','beta0','beta1','beta2'])\n",
    "print(pd_mean)\n",
    "print(pd_std)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Codes_Questions2-4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
