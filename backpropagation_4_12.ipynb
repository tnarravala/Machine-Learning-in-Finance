{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation_4_12 (Solution to Exercise 4.12)\n",
    "# Author: Matthew Dixon\n",
    "# Version: 1.0 (24.7.2019)\n",
    "# License: MIT\n",
    "# Email: matthew.dixon@iit.edu\n",
    "# Notes: tested on Mac OS X with Python 3.6 and Tensorflow 1.3.0\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# Bilokon P., Dixon M.F. and I. Halperin, Machine Learning in Finance: From Theory to Practice, Springer Graduate textbook Series, 2020. \n",
    "# This notebook is courtesy of Justin Li, Imperial College (jcl514@ic.ac.uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd \n",
    "from numpy.linalg import norm\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x*(np.sign(x)+1.)/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heaviside(x):\n",
    "    return (np.sign(x)+1.)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "ab7671c8b804e06579cfab4d74d8c6fec680eb0a"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1.+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "9644cbf438ea83c5b6797eea3d2d8d61d32c673d"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "6aba0cf2b87a1a1f8c040f3f4df19a8d752501da"
   },
   "outputs": [],
   "source": [
    "def mynorm(Z):\n",
    "    return np.sqrt(np.mean(Z**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba0305a6cb5cd493c395c105bf2b7f01442893b3"
   },
   "source": [
    "Let us consider a feed-forward architecture with an input layer, $L-1$ hidden layers and one output layer, with $K$ units in the output layer. As a result, we have $L$ sets of weights and biases $(W^{(\\ell)}, \\mathbf{b}^{(\\ell)})$ for $\\ell=1,\\dots, L$, corresponding to the layer inputs $Z^{(\\ell-1)}$ and outputs $Z^{(\\ell)}$ for $\\ell=1,\\dots, L$. Recall that each layer is an activation of a semi-affine transformation, $I^{(\\ell)}(Z^{(\\ell-1)}):=W^{(L)}Z^{(\\ell-1)}+ b^{(L)}$. The corresponding activation functions are denoted as $\\sigma^{(\\ell)}$. The activation function for the output layer is a softmax function, $\\sigma_s(x)$. \n",
    "\n",
    "Here we use the cross-entropy as the loss function, which is defined as \n",
    "$$ \\mathcal{L}:= -\\sum_{k=1}^{K}Y_{k}\\log \\hat{Y}_{k}.$$\n",
    "\n",
    "The relationship between the layers, for $\\ell\\in\\{1,\\dots, L\\}$ are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{Y} (X) & = Z^{(L)}=\\sigma_s(I^{(L)}) \\in [0,1]^{K},\\\\\n",
    "Z^{(\\ell)} & = \\sigma^{(\\ell)} \\left ( I^{(\\ell)} \\right ), ~\\ell=1,\\dots,L-1,\\\\\n",
    "Z^{(0)} & = X.\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The update rules for the weights and biases are\n",
    "\\begin{align*}\n",
    "    \\Delta W^{(\\ell)} &=  - \\gamma \\nabla_{W^{(\\ell)}}\\mathcal{L},\\\\\n",
    "    \\Delta \\mathbf{b}^{(\\ell)} &=  - \\gamma \\nabla_{\\mathbf{b}^{(\\ell)}}\\mathcal{L}.\n",
    "\\end{align*}\n",
    "We now begin the Back-Propagation.\n",
    "\n",
    "For the gradient of $\\mathcal{L}$ w.r.t. $W^{(L)}$ we have\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}^{(L)}} &= \\sum_{k=1}^{K}\\frac{\\partial \\mathcal{L}}{\\partial Z_{k}^{(L)}} \\frac{\\partial Z_{k}^{(L)}}{\\partial w_{ij}^{(L)}}\\\\\n",
    "    %%%%%%%%%%%\n",
    "    &= \\sum_{k=1}^{K}\\frac{\\partial \\mathcal{L}}{\\partial Z_{k}^{(L)}} \\sum_{m=1}^{K}\\frac{\\partial Z_{k}^{(L)}}{\\partial I_{m}^{(L)}} \\frac{\\partial I_{m}^{(L)}}{\\partial w_{ij}^{(L)}}\n",
    "\\end{align*}\n",
    "But \n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial Z_{k}^{(L)}} &= -\\frac{Y_{k}}{Z_{k}^{(L)}}\\\\\n",
    "    %%%%%%%%%%%%%%\n",
    "    \\frac{\\partial Z_{k}^{(L)}}{\\partial I_{m}^{(L)}} &= \\frac{\\partial}{\\partial I_{m}^{(L)}}[\\sigma(I^{(L)})]_{k}\\\\\n",
    "    %%%%%%%%%%%%%%\n",
    "    &= \\frac{\\partial}{\\partial I_{m}^{(L)}} \\frac{\\exp[I_{k}^{(L)}]}{\\sum_{n=1}^{K}\\exp[I_{n}^{(L)}]}\\\\\n",
    "    %%%%%%%%%%%%%%\n",
    "    &= \\begin{cases} \n",
    "    -\\frac{\\exp[I_{k}^{(L)}]}{\\sum_{n=1}^{K}\\exp[I_{n}^{(L)}]} \\frac{\\exp[I_{m}^{(L)}]}{\\sum_{n=1}^{K}\\exp[I_{n}^{(L)}]} & \\text{if } k \\neq m \\\\\n",
    "    \\frac{\\exp[I_{k}^{(L)}]}{\\sum_{n=1}^{K}\\exp[I_{n}^{(L)}]} - \\frac{\\exp[I_{k}^{(L)}]}\n",
    "    {\\sum_{n=1}^{K}\\exp[I_{n}^{(L)}]} \\frac{\\exp[I_{m}^{(L)}]}{\\sum_{n=1}^{K}\\exp[I_{n}^{(L)}]} \n",
    "    & \\text{otherwise}\n",
    "    \\end{cases}\\\\\n",
    "    %%%%%%%%%%%%%%\n",
    "    &= \\begin{cases} \n",
    "    -\\sigma_{k}\\sigma_{m}& \\text{if } k \\neq m \\\\\n",
    "     \\sigma_k(1 - \\sigma_m) & \\text{otherwise}\n",
    "    \\end{cases}\\\\\n",
    "    %%%%%%%%%%%%%%\n",
    "    &= \\sigma_k(\\delta_{km} - \\sigma_m) \\quad \\text{where} \\, \\delta_{km} \\, \\text{is the Kronecker's Delta}\\\\\n",
    "    %%%%%%%%%%%%%%\n",
    "    \\frac{\\partial I_{m}^{(L)}}{\\partial w_{ij}^{(L)}} &= \\delta_{mi}Z_{j}^{(L-1)}\\\\\n",
    "    %%%%%%%%%%%%%%\n",
    "    \\implies \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}^{(L)}} &= -\\sum_{k=1}^{K}\\frac{Y_{k}}{Z_{k}^{(L)}} \n",
    "    \\sum_{m=1}^{K} Z_{m}^{(L)}(\\delta_{km} - Z_{m}^{(L)}) \\delta_{mi}Z_{j}^{(L-1)}\\\\\n",
    "    %%%%%%%%%%%%%%\n",
    "    &= -Z_{j}^{(L-1)} \\sum_{k=1}^{K}Y_{k}  (\\delta_{ki} - Z_{i}^{(L)}) \\\\\n",
    "    %%%%%%%%%%%%%%\n",
    "    &= Z_{j}^{(L-1)} (Z_{i}^{(L)}-Y_{i})\n",
    "\\end{align*}\n",
    "Where we have used the fact that $\\sum_{k=1}^{K}Y_{k}=1$ in the last equality.\n",
    "\n",
    "Similarly for $\\mathbf{b}^{(L)}$, we have\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial b_{i}^{(L)}} &= \\sum_{k=1}^{K}\\frac{\\partial \\mathcal{L}}{\\partial Z_{k}^{(L)}} \\sum_{m=1}^{K}\\frac{\\partial Z_{k}^{(L)}}{\\partial I_{m}^{(L)}} \\frac{\\partial I_{m}^{(L)}}{\\partial b_{i}^{(L)}}\\\\\n",
    "    %%%%%%%%%%%%%%%%%%\n",
    "    &= Z_{i}^{(L)}-Y_{i}\n",
    "\\end{align*}\n",
    "It follows that\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\mathbf{b}^{(L)}}\\mathcal{L} &= Z^{(L)}-Y\\\\\n",
    "    \\nabla_{W^{(L)}}\\mathcal{L} &= \\nabla_{\\mathbf{b}^{(L)}}\\mathcal{L} \\otimes {Z^{(L-1)}}\n",
    "\\end{align*}\n",
    "Where $\\otimes$ denotes the outer product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "baff8933f9f9ab7421c873f385975555d58efcec"
   },
   "source": [
    "For the gradient of $\\mathcal{L}$ w.r.t. $W^{(L-1)}$ we have\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}^{(L-1)}} &= \\sum_{k=1}^{K}\\frac{\\partial L}{\\partial Z_{k}^{(L)}} \\frac{\\partial Z_{k}^{(L)}}{\\partial w_{ij}^{(L-1)}}\\\\\n",
    "    %%%%%%%%%%%\n",
    "    &= \\sum_{k=1}^{K}\\frac{\\partial \\mathcal{L}}{\\partial Z_{k}^{(L)}} \\sum_{m=1}^{K}\\frac{\\partial Z_{k}^{(L)}}{\\partial I_{m}^{(L)}} \\sum_{n=1}^{n^{(L-1)}} \\frac{\\partial I_{m}^{(L)}}{\\partial Z_{n}^{(L-1)}} \\sum_{p=1}^{n^{(L-1)}} \\frac{\\partial Z_{n}^{(L-1)}}{\\partial I_{p}^{(L-1)}} \\frac{\\partial I_{p}^{(L-1)}}{\\partial w_{ij}^{(L-1)}}\n",
    "\\end{align*}\n",
    "If we assume that $\\sigma^{(\\ell)}(x)=\\text{sigmoid}(x), ~\\ell \\in \\{1,\\dots, L-1\\}$ then \n",
    "\\begin{align*}\n",
    "    \\frac{\\partial I_{m}^{(L)}}{\\partial Z_{n}^{(L-1)}} &= w_{mn}^{(L)}\\\\\n",
    "    %%%%%%%%%%%%%%%%%\n",
    "    \\frac{\\partial Z_{n}^{(L-1)}}{\\partial I_{p}^{(L-1)}} &= \\frac{\\partial}{\\partial I_{p}^{(L-1)}}\\bigg(\\frac{1}{1+\\exp(-I_{n}^{(L-1)})}\\bigg)\\\\\n",
    "    %%%%%%%%%%%%%%%%%\n",
    "    &= \\frac{1}{1+\\exp(-I_{n}^{(L-1)})} \\frac{\\exp(-I_{n}^{(L-1)})}{1+\\exp(-I_{n}^{(L-1)})} \\, \\delta_{np} \\\\\n",
    "    %%%%%%%%%%%%%%%%%\n",
    "    &= Z_{n}^{(L-1)} (1-Z_{n}^{(L-1)}) \\, \\delta_{np} = \\sigma^{(L-1)}_n(1-\\sigma^{(L-1)}_n)\\delta_{np} \\\\\n",
    "    %%%%%%%%%%%%%%%%%\n",
    "    \\frac{\\partial I_{p}^{(L-1)}}{\\partial w_{ij}^{(L-1)}} &= \\delta_{pi} Z_{j}^{(L-2)} \\\\\n",
    "    %%%%%%%%%%%%%%%%%\n",
    "    \\implies \\frac{\\partial L}{\\partial w_{ij}^{(L)}} &= -\\sum_{k=1}^{K}\\frac{Y_{k}}{Z_{k}^{(L)}} \\sum_{m=1}^{K}Z_{k}^{(L)}(\\delta_{km} - Z_{m}^{(L)}) \\sum_{n=1}^{n^{(L-1)}} w_{mn}^{(L)} \\sum_{p=1}^{n^{(L-1)}} Z_{n}^{(L-1)} (1-Z_{n}^{(L-1)}) \\, \\delta_{np} \\delta_{pi} Z_{j}^{(L-2)} \\\\\n",
    "    %%%%%%%%%%%%%%%%%\n",
    "    &= -\\sum_{k=1}^{K}Y_{k} \\sum_{m=1}^{K}(\\delta_{km} - Z_{m}^{(L)}) \\sum_{n=1}^{n^{(L-1)}} w_{mn}^{(L)} Z_{n}^{(L-1)} (1-Z_{n}^{(L-1)}) \\, \\delta_{ni} Z_{j}^{(L-2)} \\\\\n",
    "    %%%%%%%%%%%%%%%%%\n",
    "    &= -\\sum_{k=1}^{K}Y_{k} \\sum_{m=1}^{K}(\\delta_{km} - Z_{m}^{(L)}) w_{mi}^{(L)} Z_{i}^{(L-2)} (1-Z_{i}^{(L-1)}) Z_{j}^{(L-2)} \\\\\n",
    "    %%%%%%%%%%%%%%%%%\n",
    "    &= -Z_{j}^{(L-2)}Z_{i}^{(L-1)}(1-Z_{i}^{(L-1)}) \\sum_{m=1}^{K} w_{mi}^{(L)} \\sum_{k=1}^{K}(\\delta_{km}Y_{k} - Z_{m}^{(L)}Y_{k}) \\\\\n",
    "    %%%%%%%%%%%%%%%%%\n",
    "    &= Z_{j}^{(L-2)}Z_{i}^{(L-1)} (1-Z_{i}^{(L-1)}) \\sum_{m=1}^{K} w_{mi}^{(L)} (Z_{m}^{(L)} - Y_{m}) \\\\\n",
    "    %%%%%%%%%%%%%%%%%\n",
    "    &= Z_{j}^{(L-2)}Z_{i}^{(L-1)} (1-Z_{i}^{(L-1)}) (Z^{(L)} - Y)^{T} \\mathbf{w}_{,i}^{(L)} \\\\\n",
    "\\end{align*}\n",
    "Similarly we have\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial b_{i}^{(L-1)}} = Z_{i}^{(L-1)} (1-Z_{i}^{(L-1)}) (Z^{(L)} - Y)^{T} \\mathbf{w}_{,i}^{(L)}. $$\n",
    "It follows that we can define the following recursion relation for the loss gradient:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla_{b^{(L-1)}}\\mathcal{L} &= Z^{(L-1)} \\circ (\\mathbf{1}-Z^{(L-1)}) \\circ \n",
    "    ({W^{(L)}}^{T} \\nabla_{b^{(L)}}\\mathcal{L}) \\\\\n",
    "    \\nabla_{W^{(L-1)}}\\mathcal{L} &= \\nabla_{b^{(L-1)}}\\mathcal{L} \\otimes Z^{(L-2)}\\\\\n",
    "    & = Z^{(L-1)} \\circ (\\mathbf{1}-Z^{(L-1)}) \\circ \n",
    "    ({W^{(L)}}^{T} \\nabla_{W^{(L)}}\\mathcal{L})\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\circ$ denotes the Hadamard Product (elementwise multiplication). This recursion relation generalizes for all layers. To see this, let the back-propagation error $\\delta^{(\\ell)}:=\\nabla_{b^{(\\ell)}}\\mathcal{L}$, and since\n",
    "\n",
    "\\begin{align*}\n",
    "\\left[\\frac{\\partial \\sigma^{(\\ell)}}{\\partial I^{(\\ell)}}\\right]_{ij}&=\\frac{\\partial \\sigma_i^{(\\ell)}}{\\partial I_j^{(\\ell)}}\\\\\n",
    "&=\\sigma_i^{(\\ell)}(1-\\sigma_i^{(\\ell)})\\delta_{ij}\\\\\n",
    "\\end{align*}\n",
    "Equivalently in matrix-vector form\n",
    "$$\\nabla_{I^{(\\ell)}} \\sigma^{(\\ell)}=\\text{diag}(\\sigma^{(\\ell)} \\circ (\\mathbf{1}-\\sigma^{(\\ell)})).$$\n",
    "\n",
    "We can write, in general, for any choice of activation function for the hidden layer,\n",
    "\n",
    "$$ \\delta^{(\\ell)}=(\\nabla_{I^{(\\ell)}} \\sigma^{(\\ell)})(W^{(\\ell+1)})^T\\delta^{(\\ell+1)}.$$\n",
    "and\n",
    "\n",
    "$$\\nabla_{W^{(\\ell)}}\\mathcal{L} = \\delta^{(\\ell)} \\otimes Z^{(\\ell-1)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of a neural network, outside of Keras, for testing backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "d5e7c8262ea14a5f1e3678534442f0c6292486f2"
   },
   "outputs": [],
   "source": [
    "def myANN(Y,Xtrain,Xpred,W01,W02,W03,b01,b02,b03):\n",
    "    # Initialization of Weights and Biases\n",
    "    W1 = copy.copy(W01)\n",
    "    W2 = copy.copy(W02)\n",
    "    W3 = copy.copy(W03)\n",
    "    b1 = copy.copy(b01)\n",
    "    b2 = copy.copy(b02)\n",
    "    b3 = copy.copy(b03)\n",
    "    # Initialize adhoc variables\n",
    "    k = 1\n",
    "    change = 999\n",
    "    # Begin Feedforward (assume learning rate is one)\n",
    "    while (change > 0.001 and k<201):\n",
    "        print(\"Iteration\", k)\n",
    "        # Hidden Layer 1\n",
    "        Z1 = relu(W1@Xtrain + b1)\n",
    "        # Hidden Layer 2\n",
    "        Z2 = sigmoid(W2@Z1 + b2)\n",
    "        # Output Layer\n",
    "        Yhat = softmax(W3@Z2 + b3)\n",
    "        # Find cross-entropy loss\n",
    "        loss = -Y@np.log(Yhat)\n",
    "        print(\"Current Loss:\",loss)\n",
    "        \n",
    "        # Find gradient of loss with respect to the weights\n",
    "        # Output Later\n",
    "        dLdb3 = Yhat - Y #\n",
    "        dLdW3 = np.outer(dLdb3,Z2)\n",
    "        # Hidden Layer 2\n",
    "        dLdb2 = (W3.T@(dLdb3))*Z2*(1-Z2)\n",
    "        dLdW2 = np.outer(dLdb2,Z1)\n",
    "        # Hidden Layer 1\n",
    "        dLdb1 = (W2.T@(dLdb2))*heaviside(W1@Xtrain + b1)\n",
    "        dLdW1 = np.outer(dLdb1,Xtrain)\n",
    "        \n",
    "        # Update Weights by Back Propagation\n",
    "        # Output Layer\n",
    "        b3 -= dLdb3 #(learning rate is one)\n",
    "        W3 -= dLdW3\n",
    "        # Hidden Layer 2\n",
    "        b2 -= dLdb2\n",
    "        W2 -= dLdW2\n",
    "        # Hidden Layer 1\n",
    "        b1 -= dLdb1\n",
    "        W1 -= dLdW1\n",
    "        \n",
    "        change = norm(dLdb1)+norm(dLdb2)+norm(dLdb3)+norm(dLdW1)+norm(dLdW2)+norm(dLdW3)\n",
    "        k+= 1\n",
    "        \n",
    "    Z1pred = W1@Xpred + b1\n",
    "    Z2pred = W2@relu(Z1pred) + b2\n",
    "    Z3pred = W3@sigmoid(Z2pred) + b3\n",
    "    Ypred = softmax(Z3pred)\n",
    "    print(\"\")\n",
    "    print(\"Summary\")\n",
    "    print(\"Target Y\", Y)\n",
    "    print(\"Fitted Ytrain\", Yhat)\n",
    "    print(\"Xpred\", Xpred)\n",
    "    print(\"Fitted Ypred\", Ypred)\n",
    "    print(\"Weight Matrix 1\", W1)\n",
    "    print(\"Bias Vector 1\", b1)\n",
    "    print(\"Weight Matrix 2\", W2)\n",
    "    print(\"Bias Vector 2\", b2)\n",
    "    print(\"Weight Matrix 3\", W3)\n",
    "    print(\"Bias Vector 3\", b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "5c41fd636007d1448a7a3d9e8481a417e771faec"
   },
   "outputs": [],
   "source": [
    "W0_1 = np.array([[0.1,0.3,0.7],[0.9,0.4,0.4]])\n",
    "b_1 = np.array([1.,1.])\n",
    "W0_2 = np.array([[0.4,0.3],[0.7,0.2]])\n",
    "b_2 = np.array([1.,1.])\n",
    "W0_3 = np.array([[0.5,0.6],[0.6,0.7],[0.3,0.2]])\n",
    "b_3 = np.array([1.,1.,1.])\n",
    "YY = np.array([1.,0.,0.])\n",
    "X_train = np.array([0.1,0.7,0.3])\n",
    "X_pred = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "9807df2b46c07bcd1f58c6e19e75b47294911283",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Current Loss: 1.0220603290496275\n",
      "Iteration 2\n",
      "Current Loss: 0.12835236020870527\n",
      "Iteration 3\n",
      "Current Loss: 0.08090078793434305\n",
      "Iteration 4\n",
      "Current Loss: 0.05953583479959617\n",
      "Iteration 5\n",
      "Current Loss: 0.04721118634517901\n",
      "Iteration 6\n",
      "Current Loss: 0.03915120608280936\n",
      "Iteration 7\n",
      "Current Loss: 0.03345605316228914\n",
      "Iteration 8\n",
      "Current Loss: 0.029213117563831773\n",
      "Iteration 9\n",
      "Current Loss: 0.025927552828981213\n",
      "Iteration 10\n",
      "Current Loss: 0.02330712020071898\n",
      "Iteration 11\n",
      "Current Loss: 0.021167867391973044\n",
      "Iteration 12\n",
      "Current Loss: 0.01938811367291565\n",
      "Iteration 13\n",
      "Current Loss: 0.017884103003570954\n",
      "Iteration 14\n",
      "Current Loss: 0.016596270340631637\n",
      "Iteration 15\n",
      "Current Loss: 0.015481081906068636\n",
      "Iteration 16\n",
      "Current Loss: 0.014505974042139025\n",
      "Iteration 17\n",
      "Current Loss: 0.013646097998312075\n",
      "Iteration 18\n",
      "Current Loss: 0.012882160242001632\n",
      "Iteration 19\n",
      "Current Loss: 0.012198950566520442\n",
      "Iteration 20\n",
      "Current Loss: 0.011584315088619651\n",
      "Iteration 21\n",
      "Current Loss: 0.011028424641181308\n",
      "Iteration 22\n",
      "Current Loss: 0.010523243891044792\n",
      "Iteration 23\n",
      "Current Loss: 0.010062139693487153\n",
      "Iteration 24\n",
      "Current Loss: 0.009639587833506902\n",
      "Iteration 25\n",
      "Current Loss: 0.009250950458533573\n",
      "Iteration 26\n",
      "Current Loss: 0.008892305077855288\n",
      "Iteration 27\n",
      "Current Loss: 0.008560311700414696\n",
      "Iteration 28\n",
      "Current Loss: 0.008252108537762037\n",
      "Iteration 29\n",
      "Current Loss: 0.007965229351517857\n",
      "Iteration 30\n",
      "Current Loss: 0.007697537377711139\n",
      "Iteration 31\n",
      "Current Loss: 0.007447172073045418\n",
      "Iteration 32\n",
      "Current Loss: 0.007212505870142702\n",
      "Iteration 33\n",
      "Current Loss: 0.006992108812929068\n",
      "Iteration 34\n",
      "Current Loss: 0.0067847194457300395\n",
      "Iteration 35\n",
      "Current Loss: 0.006589220702423551\n",
      "Iteration 36\n",
      "Current Loss: 0.006404619821317148\n",
      "Iteration 37\n",
      "Current Loss: 0.006230031522577576\n",
      "Iteration 38\n",
      "Current Loss: 0.006064663846087807\n",
      "Iteration 39\n",
      "Current Loss: 0.005907806171361446\n",
      "Iteration 40\n",
      "Current Loss: 0.005758819037026635\n",
      "Iteration 41\n",
      "Current Loss: 0.005617125452144348\n",
      "Iteration 42\n",
      "Current Loss: 0.005482203450360682\n",
      "Iteration 43\n",
      "Current Loss: 0.005353579684281697\n",
      "Iteration 44\n",
      "Current Loss: 0.005230823894365037\n",
      "Iteration 45\n",
      "Current Loss: 0.005113544116118423\n",
      "Iteration 46\n",
      "Current Loss: 0.005001382513119942\n",
      "Iteration 47\n",
      "Current Loss: 0.0048940117425445\n",
      "Iteration 48\n",
      "Current Loss: 0.004791131775457785\n",
      "Iteration 49\n",
      "Current Loss: 0.004692467106838367\n",
      "Iteration 50\n",
      "Current Loss: 0.00459776430071389\n",
      "Iteration 51\n",
      "Current Loss: 0.004506789824370946\n",
      "Iteration 52\n",
      "Current Loss: 0.004419328132686974\n",
      "Iteration 53\n",
      "Current Loss: 0.0043351799695212955\n",
      "Iteration 54\n",
      "Current Loss: 0.004254160858001673\n",
      "Iteration 55\n",
      "Current Loss: 0.004176099755647466\n",
      "Iteration 56\n",
      "Current Loss: 0.00410083785370396\n",
      "Iteration 57\n",
      "Current Loss: 0.004028227502969514\n",
      "Iteration 58\n",
      "Current Loss: 0.00395813125082976\n",
      "Iteration 59\n",
      "Current Loss: 0.0038904209763019866\n",
      "Iteration 60\n",
      "Current Loss: 0.003824977111642391\n",
      "Iteration 61\n",
      "Current Loss: 0.0037616879405767347\n",
      "Iteration 62\n",
      "Current Loss: 0.0037004489644965615\n",
      "Iteration 63\n",
      "Current Loss: 0.0036411623290665027\n",
      "Iteration 64\n",
      "Current Loss: 0.0035837363046264618\n",
      "Iteration 65\n",
      "Current Loss: 0.003528084814595028\n",
      "Iteration 66\n",
      "Current Loss: 0.0034741270067776653\n",
      "Iteration 67\n",
      "Current Loss: 0.003421786863095778\n",
      "Iteration 68\n",
      "Current Loss: 0.0033709928437758905\n",
      "Iteration 69\n",
      "Current Loss: 0.0033216775625053803\n",
      "Iteration 70\n",
      "Current Loss: 0.0032737774894514814\n",
      "Iteration 71\n",
      "Current Loss: 0.003227232679396885\n",
      "Iteration 72\n",
      "Current Loss: 0.003181986522549217\n",
      "Iteration 73\n",
      "Current Loss: 0.0031379855158476097\n",
      "Iteration 74\n",
      "Current Loss: 0.0030951790528252086\n",
      "Iteration 75\n",
      "Current Loss: 0.0030535192302974445\n",
      "Iteration 76\n",
      "Current Loss: 0.003012960670323256\n",
      "Iteration 77\n",
      "Current Loss: 0.0029734603560503995\n",
      "Iteration 78\n",
      "Current Loss: 0.0029349774801968598\n",
      "Iteration 79\n",
      "Current Loss: 0.0028974733050524813\n",
      "Iteration 80\n",
      "Current Loss: 0.0028609110329853526\n",
      "Iteration 81\n",
      "Current Loss: 0.002825255686549883\n",
      "Iteration 82\n",
      "Current Loss: 0.0027904739973753727\n",
      "Iteration 83\n",
      "Current Loss: 0.002756534303090415\n",
      "Iteration 84\n",
      "Current Loss: 0.0027234064516200034\n",
      "Iteration 85\n",
      "Current Loss: 0.0026910617122401816\n",
      "Iteration 86\n",
      "Current Loss: 0.0026594726928453115\n",
      "Iteration 87\n",
      "Current Loss: 0.0026286132629259515\n",
      "Iteration 88\n",
      "Current Loss: 0.0025984584818010896\n",
      "Iteration 89\n",
      "Current Loss: 0.0025689845316947224\n",
      "Iteration 90\n",
      "Current Loss: 0.0025401686552767293\n",
      "Iteration 91\n",
      "Current Loss: 0.0025119890973261485\n",
      "Iteration 92\n",
      "Current Loss: 0.0024844250502017327\n",
      "Iteration 93\n",
      "Current Loss: 0.002457456602833834\n",
      "Iteration 94\n",
      "Current Loss: 0.0024310646929758366\n",
      "Iteration 95\n",
      "Current Loss: 0.0024052310624723444\n",
      "Iteration 96\n",
      "Current Loss: 0.0023799382153276796\n",
      "Iteration 97\n",
      "Current Loss: 0.002355169378368623\n",
      "Iteration 98\n",
      "Current Loss: 0.0023309084643187584\n",
      "Iteration 99\n",
      "Current Loss: 0.002307140037112342\n",
      "Iteration 100\n",
      "Current Loss: 0.002283849279292448\n",
      "Iteration 101\n",
      "Current Loss: 0.002261021961345581\n",
      "Iteration 102\n",
      "Current Loss: 0.002238644412843735\n",
      "Iteration 103\n",
      "Current Loss: 0.0022167034952678523\n",
      "Iteration 104\n",
      "Current Loss: 0.002195186576402195\n",
      "Iteration 105\n",
      "Current Loss: 0.002174081506191195\n",
      "Iteration 106\n",
      "Current Loss: 0.0021533765939656634\n",
      "Iteration 107\n",
      "Current Loss: 0.0021330605869477495\n",
      "Iteration 108\n",
      "Current Loss: 0.0021131226499489808\n",
      "Iteration 109\n",
      "Current Loss: 0.0020935523461901124\n",
      "Iteration 110\n",
      "Current Loss: 0.002074339619165226\n",
      "Iteration 111\n",
      "Current Loss: 0.0020554747754891254\n",
      "Iteration 112\n",
      "Current Loss: 0.0020369484686639963\n",
      "Iteration 113\n",
      "Current Loss: 0.0020187516837080126\n",
      "Iteration 114\n",
      "Current Loss: 0.002000875722595269\n",
      "Iteration 115\n",
      "Current Loss: 0.0019833121904558916\n",
      "Iteration 116\n",
      "Current Loss: 0.001966052982488544\n",
      "Iteration 117\n",
      "Current Loss: 0.0019490902715484674\n",
      "Iteration 118\n",
      "Current Loss: 0.0019324164963639779\n",
      "Iteration 119\n",
      "Current Loss: 0.001916024350349943\n",
      "Iteration 120\n",
      "Current Loss: 0.0018999067709793184\n",
      "Iteration 121\n",
      "Current Loss: 0.0018840569296841865\n",
      "Iteration 122\n",
      "Current Loss: 0.001868468222253086\n",
      "Iteration 123\n",
      "Current Loss: 0.0018531342596963256\n",
      "Iteration 124\n",
      "Current Loss: 0.0018380488595557678\n",
      "Iteration 125\n",
      "Current Loss: 0.0018232060376306939\n",
      "Iteration 126\n",
      "Current Loss: 0.001808600000098595\n",
      "Iteration 127\n",
      "Current Loss: 0.001794225136008853\n",
      "Iteration 128\n",
      "Current Loss: 0.0017800760101280703\n",
      "Iteration 129\n",
      "Current Loss: 0.0017661473561193636\n",
      "Iteration 130\n",
      "Current Loss: 0.0017524340700357335\n",
      "Iteration 131\n",
      "Current Loss: 0.0017389312041129511\n",
      "Iteration 132\n",
      "Current Loss: 0.0017256339608448688\n",
      "Iteration 133\n",
      "Current Loss: 0.001712537687324052\n",
      "Iteration 134\n",
      "Current Loss: 0.0016996378698401003\n",
      "Iteration 135\n",
      "Current Loss: 0.0016869301287143444\n",
      "Iteration 136\n",
      "Current Loss: 0.001674410213365405\n",
      "Iteration 137\n",
      "Current Loss: 0.0016620739975900956\n",
      "Iteration 138\n",
      "Current Loss: 0.001649917475049159\n",
      "Iteration 139\n",
      "Current Loss: 0.0016379367549498873\n",
      "Iteration 140\n",
      "Current Loss: 0.001626128057911233\n",
      "Iteration 141\n",
      "Current Loss: 0.001614487712008471\n",
      "Iteration 142\n",
      "Current Loss: 0.0016030121489822463\n",
      "Iteration 143\n",
      "Current Loss: 0.001591697900608184\n",
      "Iteration 144\n",
      "Current Loss: 0.0015805415952184626\n",
      "Iteration 145\n",
      "Current Loss: 0.001569539954366085\n",
      "Iteration 146\n",
      "Current Loss: 0.0015586897896275864\n",
      "Iteration 147\n",
      "Current Loss: 0.0015479879995356999\n",
      "Iteration 148\n",
      "Current Loss: 0.0015374315666371642\n",
      "Iteration 149\n",
      "Current Loss: 0.0015270175546686453\n",
      "Iteration 150\n",
      "Current Loss: 0.0015167431058457383\n",
      "Iteration 151\n",
      "Current Loss: 0.0015066054382599117\n",
      "Iteration 152\n",
      "Current Loss: 0.0014966018433795916\n",
      "Iteration 153\n",
      "Current Loss: 0.00148672968364602\n",
      "Iteration 154\n",
      "Current Loss: 0.0014769863901669855\n",
      "Iteration 155\n",
      "Current Loss: 0.0014673694604977285\n",
      "Iteration 156\n",
      "Current Loss: 0.001457876456506448\n",
      "Iteration 157\n",
      "Current Loss: 0.0014485050023240573\n",
      "Iteration 158\n",
      "Current Loss: 0.0014392527823696138\n",
      "Iteration 159\n",
      "Current Loss: 0.0014301175394514048\n",
      "Iteration 160\n",
      "Current Loss: 0.0014210970729397854\n",
      "Iteration 161\n",
      "Current Loss: 0.0014121892370065287\n",
      "Iteration 162\n",
      "Current Loss: 0.0014033919389312309\n",
      "Iteration 163\n",
      "Current Loss: 0.0013947031374693126\n",
      "Iteration 164\n",
      "Current Loss: 0.0013861208412796022\n",
      "Iteration 165\n",
      "Current Loss: 0.0013776431074100461\n",
      "Iteration 166\n",
      "Current Loss: 0.0013692680398371996\n",
      "Iteration 167\n",
      "Current Loss: 0.0013609937880588205\n",
      "Iteration 168\n",
      "Current Loss: 0.0013528185457365566\n",
      "Iteration 169\n",
      "Current Loss: 0.001344740549387827\n",
      "Iteration 170\n",
      "Current Loss: 0.0013367580771219986\n",
      "Iteration 171\n",
      "Current Loss: 0.0013288694474230729\n",
      "Iteration 172\n",
      "Current Loss: 0.0013210730179728718\n",
      "Iteration 173\n",
      "Current Loss: 0.001313367184517497\n",
      "Iteration 174\n",
      "Current Loss: 0.0013057503797703827\n",
      "Iteration 175\n",
      "Current Loss: 0.0012982210723550514\n",
      "Iteration 176\n",
      "Current Loss: 0.0012907777657823398\n",
      "Iteration 177\n",
      "Current Loss: 0.0012834189974637565\n",
      "Iteration 178\n",
      "Current Loss: 0.0012761433377576328\n",
      "Iteration 179\n",
      "Current Loss: 0.0012689493890462823\n",
      "Iteration 180\n",
      "Current Loss: 0.0012618357848454957\n",
      "Iteration 181\n",
      "Current Loss: 0.0012548011889429252\n",
      "Iteration 182\n",
      "Current Loss: 0.0012478442945652382\n",
      "Iteration 183\n",
      "Current Loss: 0.001240963823571148\n",
      "Iteration 184\n",
      "Current Loss: 0.0012341585256733197\n",
      "Iteration 185\n",
      "Current Loss: 0.0012274271776832534\n",
      "Iteration 186\n",
      "Current Loss: 0.001220768582782257\n",
      "Iteration 187\n",
      "Current Loss: 0.0012141815698143883\n",
      "Iteration 188\n",
      "Current Loss: 0.0012076649926029235\n",
      "Iteration 189\n",
      "Current Loss: 0.0012012177292883449\n",
      "Iteration 190\n",
      "Current Loss: 0.001194838681687179\n",
      "Iteration 191\n",
      "Current Loss: 0.0011885267746719063\n",
      "Iteration 192\n",
      "Current Loss: 0.0011822809555674892\n",
      "Iteration 193\n",
      "Current Loss: 0.0011761001935705244\n",
      "Iteration 194\n",
      "Current Loss: 0.001169983479183563\n",
      "Iteration 195\n",
      "Current Loss: 0.0011639298236681586\n",
      "Iteration 196\n",
      "Current Loss: 0.001157938258513971\n",
      "Iteration 197\n",
      "Current Loss: 0.0011520078349248114\n",
      "Iteration 198\n",
      "Current Loss: 0.0011461376233192957\n",
      "Iteration 199\n",
      "Current Loss: 0.0011403267128478804\n",
      "Iteration 200\n",
      "Current Loss: 0.0011345742109229449\n",
      "\n",
      "Summary\n",
      "Target Y [1. 0. 0.]\n",
      "Fitted Ytrain [9.98866069e-01 5.78166789e-04 5.55764036e-04]\n",
      "Xpred [0.1 0.7 0.3]\n",
      "Fitted Ypred [9.98871758e-01 5.75252184e-04 5.52990114e-04]\n",
      "Weight Matrix 1 [[0.11550445 0.40853113 0.74651334]\n",
      " [0.90847057 0.45929397 0.4254117 ]]\n",
      "Bias Vector 1 [1.15504447 1.08470567]\n",
      "Weight Matrix 2 [[0.601487   0.50195391]\n",
      " [0.87786731 0.37852945]]\n",
      "Bias Vector 2 [1.13024873 1.11531658]\n",
      "Weight Matrix 3 [[ 2.09832509  2.23643944]\n",
      " [-0.31759129 -0.24000834]\n",
      " [-0.38073379 -0.49643109]]\n",
      "Bias Vector 3 [ 2.76681291 -0.01687933  0.25006642]\n"
     ]
    }
   ],
   "source": [
    "myANN(YY,X_train,X_pred,W0_1,W0_2,W0_3,b_1,b_2,b_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare result with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "3452568af4e2b0eef09b05faf226e7ca100d4340"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "c3022e1a7b78fbe0ca1bf116e3e96882b4f2a7c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.1, 0.9],\n",
       "        [0.3, 0.4],\n",
       "        [0.7, 0.4]], dtype=float32),\n",
       " array([1., 1.], dtype=float32),\n",
       " array([[0.4, 0.7],\n",
       "        [0.3, 0.2]], dtype=float32),\n",
       " array([1., 1.], dtype=float32),\n",
       " array([[0.5, 0.6, 0.3],\n",
       "        [0.6, 0.7, 0.2]], dtype=float32),\n",
       " array([1., 1., 1.], dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=3, activation='relu', weights = [W0_1.T,b_1]))\n",
    "model.add(Dense(2, activation='sigmoid', weights = [W0_2.T,b_2]))\n",
    "model.add(Dense(3, activation='softmax', weights = [W0_3.T,b_3]))\n",
    "# Compile Model\n",
    "sgd = optimizers.SGD(lr=1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['categorical_crossentropy'])\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "67897067fabc2f208b360a454d5a524940d34f2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 1.0221 - categorical_crossentropy: 1.0221\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1284 - categorical_crossentropy: 0.1284\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0809 - categorical_crossentropy: 0.0809\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0595 - categorical_crossentropy: 0.0595\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0472 - categorical_crossentropy: 0.0472\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0392 - categorical_crossentropy: 0.0392\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0335 - categorical_crossentropy: 0.0335\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0292 - categorical_crossentropy: 0.0292\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0259 - categorical_crossentropy: 0.0259\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0233 - categorical_crossentropy: 0.0233\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0212 - categorical_crossentropy: 0.0212\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0194 - categorical_crossentropy: 0.0194\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0179 - categorical_crossentropy: 0.0179\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0166 - categorical_crossentropy: 0.0166\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0155 - categorical_crossentropy: 0.0155\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0145 - categorical_crossentropy: 0.0145\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0136 - categorical_crossentropy: 0.0136\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0129 - categorical_crossentropy: 0.0129\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0122 - categorical_crossentropy: 0.0122\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0116 - categorical_crossentropy: 0.0116\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0110 - categorical_crossentropy: 0.0110\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0105 - categorical_crossentropy: 0.0105\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0101 - categorical_crossentropy: 0.0101\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0096 - categorical_crossentropy: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0093 - categorical_crossentropy: 0.0093\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0089 - categorical_crossentropy: 0.0089\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0086 - categorical_crossentropy: 0.0086\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0083 - categorical_crossentropy: 0.0083\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0080 - categorical_crossentropy: 0.0080\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0077 - categorical_crossentropy: 0.0077\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0074 - categorical_crossentropy: 0.0074\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0072 - categorical_crossentropy: 0.0072\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0070 - categorical_crossentropy: 0.0070\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0068 - categorical_crossentropy: 0.0068\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0066 - categorical_crossentropy: 0.0066\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0064 - categorical_crossentropy: 0.0064\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0062 - categorical_crossentropy: 0.0062\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0061 - categorical_crossentropy: 0.0061\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0059 - categorical_crossentropy: 0.0059\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0058 - categorical_crossentropy: 0.0058\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0056 - categorical_crossentropy: 0.0056\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0055 - categorical_crossentropy: 0.0055\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0054 - categorical_crossentropy: 0.0054\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0052 - categorical_crossentropy: 0.0052\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0051 - categorical_crossentropy: 0.0051\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0050 - categorical_crossentropy: 0.0050\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0049 - categorical_crossentropy: 0.0049\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0048 - categorical_crossentropy: 0.0048\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0047 - categorical_crossentropy: 0.0047\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0046 - categorical_crossentropy: 0.0046\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0045 - categorical_crossentropy: 0.0045\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0044 - categorical_crossentropy: 0.0044\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0043 - categorical_crossentropy: 0.0043\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0043 - categorical_crossentropy: 0.0043\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0042 - categorical_crossentropy: 0.0042\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0041 - categorical_crossentropy: 0.0041\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0040 - categorical_crossentropy: 0.0040\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0040 - categorical_crossentropy: 0.0040\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0039 - categorical_crossentropy: 0.0039\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0038 - categorical_crossentropy: 0.0038\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0038 - categorical_crossentropy: 0.0038\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0037 - categorical_crossentropy: 0.0037\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0036 - categorical_crossentropy: 0.0036\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0036 - categorical_crossentropy: 0.0036\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0035 - categorical_crossentropy: 0.0035\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0035 - categorical_crossentropy: 0.0035\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0034 - categorical_crossentropy: 0.0034\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0034 - categorical_crossentropy: 0.0034\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0033 - categorical_crossentropy: 0.0033\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0033 - categorical_crossentropy: 0.0033\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0032 - categorical_crossentropy: 0.0032\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0032 - categorical_crossentropy: 0.0032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0031 - categorical_crossentropy: 0.0031\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0031 - categorical_crossentropy: 0.0031\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0031 - categorical_crossentropy: 0.0031\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0030 - categorical_crossentropy: 0.0030\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0030 - categorical_crossentropy: 0.0030\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0029 - categorical_crossentropy: 0.0029\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0029 - categorical_crossentropy: 0.0029\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0029 - categorical_crossentropy: 0.0029\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0028 - categorical_crossentropy: 0.0028\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0028 - categorical_crossentropy: 0.0028\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0028 - categorical_crossentropy: 0.0028\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0027 - categorical_crossentropy: 0.0027\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0027 - categorical_crossentropy: 0.0027\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0027 - categorical_crossentropy: 0.0027\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0026 - categorical_crossentropy: 0.0026\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0026 - categorical_crossentropy: 0.0026\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0026 - categorical_crossentropy: 0.0026\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0025 - categorical_crossentropy: 0.0025\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0025 - categorical_crossentropy: 0.0025\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0025 - categorical_crossentropy: 0.0025\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0025 - categorical_crossentropy: 0.0025\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0024 - categorical_crossentropy: 0.0024\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0024 - categorical_crossentropy: 0.0024\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0024 - categorical_crossentropy: 0.0024\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0024 - categorical_crossentropy: 0.0024\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0023 - categorical_crossentropy: 0.0023\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0023 - categorical_crossentropy: 0.0023\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0023 - categorical_crossentropy: 0.0023\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0023 - categorical_crossentropy: 0.0023\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0022 - categorical_crossentropy: 0.0022\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0022 - categorical_crossentropy: 0.0022\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0022 - categorical_crossentropy: 0.0022\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0022 - categorical_crossentropy: 0.0022\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0022 - categorical_crossentropy: 0.0022\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0021 - categorical_crossentropy: 0.0021\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0021 - categorical_crossentropy: 0.0021\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0021 - categorical_crossentropy: 0.0021\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0021 - categorical_crossentropy: 0.0021\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0021 - categorical_crossentropy: 0.0021\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0020 - categorical_crossentropy: 0.0020\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0020 - categorical_crossentropy: 0.0020\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0020 - categorical_crossentropy: 0.0020\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0020 - categorical_crossentropy: 0.0020\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0020 - categorical_crossentropy: 0.0020\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0019 - categorical_crossentropy: 0.0019\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0019 - categorical_crossentropy: 0.0019\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0019 - categorical_crossentropy: 0.0019\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0019 - categorical_crossentropy: 0.0019\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0019 - categorical_crossentropy: 0.0019\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0019 - categorical_crossentropy: 0.0019\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0019 - categorical_crossentropy: 0.0019\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0018 - categorical_crossentropy: 0.0018\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0018 - categorical_crossentropy: 0.0018\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0018 - categorical_crossentropy: 0.0018\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0018 - categorical_crossentropy: 0.0018\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0018 - categorical_crossentropy: 0.0018\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0018 - categorical_crossentropy: 0.0018\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0018 - categorical_crossentropy: 0.0018\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0017 - categorical_crossentropy: 0.0017\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0017 - categorical_crossentropy: 0.0017\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0017 - categorical_crossentropy: 0.0017\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017 - categorical_crossentropy: 0.0017\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017 - categorical_crossentropy: 0.0017\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017 - categorical_crossentropy: 0.0017\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0017 - categorical_crossentropy: 0.0017\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016 - categorical_crossentropy: 0.0016\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016 - categorical_crossentropy: 0.0016\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0016 - categorical_crossentropy: 0.0016\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0016 - categorical_crossentropy: 0.0016\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016 - categorical_crossentropy: 0.0016\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016 - categorical_crossentropy: 0.0016\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016 - categorical_crossentropy: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016 - categorical_crossentropy: 0.0016\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016 - categorical_crossentropy: 0.0016\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015 - categorical_crossentropy: 0.0015\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015 - categorical_crossentropy: 0.0015\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015 - categorical_crossentropy: 0.0015\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0015 - categorical_crossentropy: 0.0015\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015 - categorical_crossentropy: 0.0015\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015 - categorical_crossentropy: 0.0015\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015 - categorical_crossentropy: 0.0015\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0015 - categorical_crossentropy: 0.0015\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015 - categorical_crossentropy: 0.0015\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015 - categorical_crossentropy: 0.0015\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0013 - categorical_crossentropy: 0.0013\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011 - categorical_crossentropy: 0.0011\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011 - categorical_crossentropy: 0.0011\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0011 - categorical_crossentropy: 0.0011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1195dddd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train.reshape((1,3)), YY.reshape((1,3)), epochs=200, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "1dc231803e034841a955e31c2ce2e3d68b9eced1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9887174e-01, 5.7524466e-04, 5.5298163e-04]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_pred.reshape((1,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "889e04a3f4b08b56ba9dbc0a5771186bd40c23ce"
   },
   "source": [
    "###  Keras Feedforward Results (Note that the results are transposed in order to match Keras' output style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "8a0592357283bc20cd7cd47d40132b49a2cbb160"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.11551907, 0.9084813 ],\n",
       "        [0.40863392, 0.4593696 ],\n",
       "        [0.74655724, 0.42544398]], dtype=float32),\n",
       " array([1.155191 , 1.0848131], dtype=float32),\n",
       " array([[0.6016261 , 0.87797874],\n",
       "        [0.50208974, 0.37863797]], dtype=float32),\n",
       " array([1.1303287, 1.1153796], dtype=float32),\n",
       " array([[ 2.0983126 , -0.3175866 , -0.38072622],\n",
       "        [ 2.2364147 , -0.23999722, -0.49641728]], dtype=float32),\n",
       " array([ 2.766754  , -0.01685131,  0.25009623], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
