{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyHodELl22w8"
   },
   "outputs": [],
   "source": [
    "# Interaction_5_6(Solutions to Exercise 5.6)\n",
    "# Author: Matthew Dixon\n",
    "# Version: 1.0 (08.09.2019)\n",
    "# License: MIT\n",
    "# Email: matthew.dixon@iit.edu\n",
    "# Notes: tested on Mac OS X with Python 3.6\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# Dixon M.F. I. Halperin and P. Bilokon, Machine Learning in Finance: From Theory to Practice, Springer Graduate textbook Series, 2020. \n",
    "# Solution courtesy of Shengnan Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "4x-ll3tz22xG",
    "outputId": "977558a9-c7e4-44d5-ff56-fcad6d40464a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.diagnostic as tds\n",
    "from statsmodels.api import add_constant\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80ePasDJ22xM"
   },
   "source": [
    "# Simple Data Generation Process (DGP)\n",
    "\n",
    "$Y=X_1+X_2 + X_1X_2+\\epsilon, ~X_1, X_2 \\sim N(0,1,), \\epsilon \\sim N(0,\\sigma_n^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8uuyzDX22xN"
   },
   "outputs": [],
   "source": [
    "M = 5000\n",
    "np.random.seed(7)\n",
    "X = np.zeros(shape=(M,2))\n",
    "sigma_n = 0.01\n",
    "X[:int(M/2),0]= np.random.randn(int(M/2))\n",
    "X[:int(M/2),1]= np.random.randn(int(M/2))\n",
    "\n",
    "X[int(M/2):,0]= -X[:int(M/2),0]\n",
    "X[int(M/2):,1]= -X[:int(M/2),1]\n",
    "\n",
    "eps= np.random.randn(M)\n",
    "Y= 1.0*X[:,0] + 1.0*X[:,1] + 1.0*X[:,0]*X[:,1] + sigma_n*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7za5g8zX22xf"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gCFePMj22x3"
   },
   "source": [
    "# Compare with a FFW Neural Network with one hidden layer (tanh activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From running the code, it is evident that increasing the number of hidden units improves the interpretability of the interaction term.  Put another way, as the number of hidden units increases, the mean of the true interaction term converges to 1, and the standard deviation of the interaction term decreases. Also, the mean of the false interaction terms converge to zero with increasing confidence. \n",
    "\n",
    "For reference, below is a table that shows the mean and standard deviation of the interaction term as the number of hidden units is increased.  This table only includes one hidden layer with tanh activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2TKVhEa22x8"
   },
   "source": [
    "## Compute the sensitivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yeNHNr4E22x8"
   },
   "outputs": [],
   "source": [
    "# Assume that the activation function is tanh\n",
    "def sensitivities(lm, X):\n",
    "    \n",
    "    W1=lm.model.get_weights()[0]\n",
    "    b1=lm.model.get_weights()[1]\n",
    "    W2=lm.model.get_weights()[2]\n",
    "    b2=lm.model.get_weights()[3]\n",
    "    \n",
    "    \n",
    "    M = np.shape(X)[0]\n",
    "    p = np.shape(X)[1]\n",
    "\n",
    "    beta=np.array([0]*M*(p+1), dtype='float32').reshape(M,p+1)\n",
    "    beta_interact=np.array([0]*M*p*p, dtype='float32').reshape(M,p,p)\n",
    "    \n",
    "    beta[:,0]= (np.dot(np.transpose(W2),np.tanh(b1)) + b2)[0] # intercept \\beta_0= F_{W,b}(0)\n",
    "    for i in range(M):\n",
    " \n",
    "      Z1 = np.tanh(np.dot(np.transpose(W1),np.transpose(X[i,])) + b1)\n",
    "      \n",
    "      D = np.diag(1-Z1**2) \n",
    "      D_prime =np.diag(-2*Z1*(1-Z1**2))   # needed for interaction term     \n",
    "      #D = np.diag(np.sign(Z1))  \n",
    "        \n",
    "      for j in range(p):  \n",
    "          beta[i,j+1]=np.dot(np.transpose(W2),np.dot(D,W1[j]))\n",
    "          #interaction term \n",
    "          for k in range(p):\n",
    "            beta_interact[i,j,k]=np.dot(np.transpose(W2),np.dot(np.diag(W1[j]), np.dot(D_prime,W1[k]))) # Hessian matrix  \n",
    "    \n",
    "            \n",
    "    return(beta, beta_interact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GR6YYZDhtTnZ"
   },
   "source": [
    "## Increasing the number of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1IqKZmc22x3"
   },
   "outputs": [],
   "source": [
    "# number of hidden neurons\n",
    "N= [10,20,50,100,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "u_39CWbt22x5",
    "outputId": "840cccb2-a4b8-4619-f42a-4d6b23142579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number_of_neurons       H11       H12       H21       H22\n",
      "0               10.0 -0.026601  0.970886  0.970886 -0.018296\n",
      "1               20.0  0.010413  1.029390  1.029390  0.016088\n",
      "2               50.0  0.003500  1.005061  1.005061 -0.000847\n",
      "3              100.0 -0.002594  0.994979  0.994979 -0.001575\n",
      "4              200.0  0.001534  0.998311  0.998311 -0.002615\n",
      "   number_of_neurons       H11       H12       H21       H22\n",
      "0               10.0  0.277478  0.266668  0.266668  0.254695\n",
      "1               20.0  0.094886  0.090346  0.090346  0.090399\n",
      "2               50.0  0.079534  0.081473  0.081473  0.085492\n",
      "3              100.0  0.053773  0.055478  0.055478  0.059201\n",
      "4              200.0  0.057174  0.059285  0.059285  0.063547\n"
     ]
    }
   ],
   "source": [
    "array_mean_inter = []\n",
    "array_std_inter = []\n",
    "for k in range(len(N)):\n",
    "  n = N[k]\n",
    "  # with non-linear activation\n",
    "  def linear_NN1_model_act(l1_reg=0.0):    \n",
    "      model = Sequential()\n",
    "      model.add(Dense(n, input_dim=2, kernel_initializer='normal', activation='tanh'))\n",
    "      model.add(Dense(1, kernel_initializer='normal')) \n",
    "      model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "      return model\n",
    "    \n",
    "  lm = KerasRegressor(build_fn=linear_NN1_model_act, epochs=100, batch_size=10, verbose=0, callbacks=[es])\n",
    "  lm.fit(X,Y)\n",
    "  beta, beta_inter=sensitivities(lm, X)\n",
    "  array_mean_inter.append(int(n))\n",
    "  array_mean_inter.extend(np.concatenate(np.mean(beta_inter, axis=0).tolist()))\n",
    "  array_std_inter.append(int(n))\n",
    "  array_std_inter.extend(np.concatenate(np.std(beta_inter, axis=0).tolist()))\n",
    "  \n",
    "pd_mean_inter = pd.DataFrame(np.reshape(array_mean_inter,(len(N),5)),columns=['number_of_neurons','H11','H12','H21','H22'])\n",
    "pd_std_inter = pd.DataFrame(np.reshape(array_std_inter,(len(N),5)),columns=['number_of_neurons','H11','H12','H21','H22'])\n",
    "print(pd_mean_inter)\n",
    "print(pd_std_inter)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Codes_Exercise6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
